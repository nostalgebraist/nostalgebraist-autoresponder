{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_side_judgments",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "knb30W3D_z7m"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpCgleeaGbm-"
      },
      "source": [
        "%tensorflow_version 1.x  # needed on Google Colab to use tf1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OOtc6tvtRaP"
      },
      "source": [
        "TRAIN_SENTIMENT = False  # to train the sentiment model instead of the selector model, set to True and re-run notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vb9qYnM_-CW"
      },
      "source": [
        "from google.colab import auth  # needed on Google Colab to use gsutil\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = \"\"  # INSERT YOUR GOOGLE CLOUD PROJECT ID HERE\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLwmtx93AA4G"
      },
      "source": [
        "%cd /\n",
        "!git clone https://github.com/nostalgebraist/nostalgebraist-autoresponder.git\n",
        "    \n",
        "%cd /nostalgebraist-autoresponder/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moiE1oTirCrN"
      },
      "source": [
        "`config_file_gs_uri` should be a GS path to the private file `config.json` loaded by the `BotSpecificConstants` class.\n",
        "\n",
        "I store it in GS, but in principle you can store it wherever you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptrEhp2q7zH"
      },
      "source": [
        "config_file_gs_uri = \"\"\n",
        "\n",
        "!gsutil cp {config_file_gs_uri} ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21JZEG6HB9XK"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "from autoresponder_config import BUCKET_NAME, gs_command_get_model, gs_command_get_encoder, model_path\n",
        "\n",
        "ckpt_dir = \"/\" + model_path.rpartition(\"/\")[0]\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "  subprocess.check_output(gs_command_get_encoder, shell=True)\n",
        "  subprocess.check_output(gs_command_get_model, shell=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65bE0i62AyFw"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(\"gpt-2/\")\n",
        "sys.path.append(\"gpt-2/src\")\n",
        "sys.path.append(\"selector_model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53BJ--5wCdLR"
      },
      "source": [
        "import encoder\n",
        "enc = encoder.get_encoder_from_path(ckpt_dir, eot_workaround=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgMiW7PLAGgb"
      },
      "source": [
        "!gsutil cp gs://{BUCKET_NAME}/selector_training_data.pkl.gz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVBE7pTAKfo"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "data_path = \"selector_training_data.pkl.gz\"\n",
        "with open(data_path, \"rb\") as f:\n",
        "    selector_training_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nHVhbQgsWg9"
      },
      "source": [
        "Data prep\n",
        "\n",
        "TODO: clean up this part of the code (and move out of notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ281NB8AoPt"
      },
      "source": [
        "import re\n",
        "from autoresponder_static import *\n",
        "\n",
        "def inverse_format_post_for_api(post):\n",
        "    if post.startswith(\"<p>\"):\n",
        "        post = post[len(\"<p>\"):]\n",
        "    if post.endswith(\"</p>\"):\n",
        "        post = post[:-len(\"</p>\")]\n",
        "    post = re.sub(r\"</p><p>\", \"\\n\", post)\n",
        "    post = re.sub(r\"<br>\", \"\\n\", post)\n",
        "    return post\n",
        "\n",
        "def make_train_data(ids_to_reward_data,\n",
        "                    continuation_only=True,\n",
        "                    prompt_as_col=True,\n",
        "                    v8_ts=True,\n",
        "                    v10_ts=True,\n",
        "                    nov_2020_new_way=True,\n",
        "                    min_tok_len=None,\n",
        "                    exclude_unames={\"bukbot\", \"enriquebot\"},\n",
        "                    exclude_substrings={\"graph of my mood\", \n",
        "                                        T_CHAR+\"#quotes<|\",\n",
        "                                        }\n",
        "                    ):\n",
        "    train_data = []\n",
        "    cols=[\"id\", \"timestamp\", \"text\", \"note_count\", ]\n",
        "    if prompt_as_col:\n",
        "      cols.append(\"prompt\")\n",
        "    if v8_ts:\n",
        "      cols.append(\"v8_timestamp\")\n",
        "    if v10_ts:\n",
        "      cols.append(\"v10_timestamp\")\n",
        "    if nov_2020_new_way:\n",
        "      cols.extend([\"is_orig\", \"is_ask\", \"is_reply\", \"is_reblog\"])\n",
        "\n",
        "    for k, v in ids_to_reward_data.items():\n",
        "      if v.get(\"note_count\") is None or v.get('continuation') is None:\n",
        "        continue\n",
        "      if continuation_only:\n",
        "        train_data.append([k, v[\"timestamp\"], v[\"continuation\"], v[\"note_count\"]])\n",
        "      else:\n",
        "        train_data.append([k, v[\"timestamp\"], \" \".join(v[\"prompt\"].split(\" \")[-64:]) + v[\"continuation\"], v[\"note_count\"]])\n",
        "      if prompt_as_col:\n",
        "        train_data[-1].append(v[\"prompt\"])\n",
        "        \n",
        "      if v8_ts:\n",
        "        train_data[-1].append(v[\"v8_timestamp\"])\n",
        "      if v8_ts:\n",
        "        train_data[-1].append(v[\"v10_timestamp\"])\n",
        "\n",
        "      if nov_2020_new_way:\n",
        "        train_data[-1].extend([v[c] for c in [\"is_orig\", \"is_ask\", \"is_reply\", \"is_reblog\"]])\n",
        "        \n",
        "    train_data = pd.DataFrame(train_data, columns=cols)\n",
        "\n",
        "    train_data.text = train_data.text.apply(inverse_format_post_for_api)\n",
        "    train_data.text = train_data.text.apply(lambda s: s.lstrip(\"\\n\"))\n",
        "\n",
        "    if min_tok_len is not None:\n",
        "      tok_len = train_data.text.apply(lambda s: len(enc.encode(s)))\n",
        "      train_data = train_data[tok_len >= min_tok_len]\n",
        "\n",
        "    for uname in exclude_unames:\n",
        "      nbefore=len(train_data)\n",
        "      train_data = train_data[train_data.prompt.apply(lambda s: uname+Q_CHAR not in s)]\n",
        "      nafter=len(train_data)\n",
        "      print(f\"excluding {nbefore-nafter} with uname {repr(uname)}\")\n",
        "\n",
        "    for subs in exclude_substrings:\n",
        "      nbefore=len(train_data)\n",
        "      train_data = train_data[train_data.text.apply(lambda s: subs not in s)]\n",
        "      nafter=len(train_data)\n",
        "      print(f\"excluding {nbefore-nafter} with substring {repr(subs)}\")\n",
        "    return train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKfJZqPIAteG"
      },
      "source": [
        "# vs 16929 | vs 16910\n",
        "train_data = make_train_data(selector_training_data, continuation_only=True, )\n",
        "train_data = train_data[train_data.is_reblog == False]   # 2/7/21\n",
        "train_data.note_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8WYgSzBBkF"
      },
      "source": [
        "import re\n",
        "V10_ASK_CHAR = \"要\"\n",
        "\n",
        "def find_all_control_chars_chinese(text,\n",
        "                               incl_number=True  # ignored\n",
        "                               ):\n",
        "    results = []\n",
        "    control_chars = [Q_CHAR, A_CHAR, UNAME_CHAR, ORIG_POST_CHAR_CHINESE, V10_ASK_CHAR] # no tchar\n",
        "    rx = f\"({UNAME_CHAR}[^{Q_CHAR}]+{Q_CHAR}|{A_CHAR}|{ORIG_POST_CHAR_CHINESE}|{V10_ASK_CHAR})\"\n",
        "    for m in re.finditer(rx, text):\n",
        "        results.append((m.group(1), m.span(1)[0]))\n",
        "    return results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBGlq00-BGUw"
      },
      "source": [
        "def count_control_char_gaps(doc, forumlike=False):\n",
        "    if forumlike:\n",
        "        chars = find_control_chars_forumlike(doc)\n",
        "    else:\n",
        "        chars = find_all_control_chars_chinese(doc)\n",
        "    \n",
        "    gaps = []\n",
        "    for char1, char2 in zip(chars[:-1], chars[1:]):\n",
        "        gap = char2[1] - (char1[1] + len(char1[0]))\n",
        "        gaps.append(gap)\n",
        "        \n",
        "    return gaps\n",
        "\n",
        "def count_small_control_char_gaps(doc, forumlike=False):\n",
        "    cutoff = 0 if forumlike else 2\n",
        "    gaps = count_control_char_gaps(doc, forumlike)\n",
        "    return sum([gap <= cutoff for gap in gaps])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvNpYkwYBHp8"
      },
      "source": [
        "train_data[\"n_small_gaps\"] = train_data.prompt.apply(lambda p: count_small_control_char_gaps(p, forumlike=False))\n",
        "train_data[\"has_small_gaps\"] = (train_data.n_small_gaps > 0)\n",
        "\n",
        "display(train_data.has_small_gaps.agg([\"count\", \"sum\", \"mean\"]))\n",
        "\n",
        "train_data = train_data[train_data.has_small_gaps==False]\n",
        "\n",
        "display(train_data.has_small_gaps.agg([\"count\", \"sum\", \"mean\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7lfmJVuBIo-"
      },
      "source": [
        "SEPARATE_REBLOGS_IN_FINALCHAR = True\n",
        "\n",
        "cchars_prompt = train_data.prompt.apply(lambda s: [tup[0] for tup in sorted(find_all_control_chars_chinese(s), key=lambda tup_: tup_[1])])\n",
        "\n",
        "cats=cchars_prompt.apply(\n",
        "    lambda l: \"orig\" if l[0]==ORIG_POST_CHAR_CHINESE else \n",
        "    (\"reblog_orig\" if l[0]==A_CHAR else \n",
        "     (\"ask\" if (l[0].startswith(UNAME_CHAR) or l[0].startswith(V10_ASK_CHAR)) and len(l)==2 else \"reblog_ask\"))\n",
        "    )\n",
        "\n",
        "train_data[\"control_cat\"] = cats\n",
        "\n",
        "if SEPARATE_REBLOGS_IN_FINALCHAR:\n",
        "  prompt_finalchar = cats.apply(lambda s: ORIG_POST_CHAR_CHINESE if s==\"orig\" else (f\"{A_CHAR}a\" if s==\"ask\" else f\"{A_CHAR}r\"))\n",
        "else:\n",
        "  prompt_finalchar = train_data.prompt.apply(lambda s: s[-1])\n",
        "\n",
        "train_data[\"prompt_finalchar\"] = prompt_finalchar\n",
        "\n",
        "display(train_data.prompt_finalchar.value_counts(normalize=True))\n",
        "display(train_data.prompt_finalchar.value_counts(normalize=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INsc9mlABJ-6"
      },
      "source": [
        "cchars_prompt = train_data.prompt.apply(lambda s: [tup[0] for tup in sorted(find_all_control_chars_chinese(s), key=lambda tup_: tup_[1])])\n",
        "\n",
        "cats=cchars_prompt.apply(\n",
        "    lambda l: \"orig\" if l[0]==ORIG_POST_CHAR_CHINESE else \n",
        "    (\"reblog_orig\" if l[0]==A_CHAR else \n",
        "     (\"ask\" if (l[0].startswith(UNAME_CHAR) or l[0].startswith(V10_ASK_CHAR)) and len(l)==2 else \"reblog_ask\"))\n",
        "    )\n",
        "\n",
        "train_data[\"control_cat\"] = cats\n",
        "\n",
        "display(train_data[train_data.note_count<=50].groupby(\"prompt_finalchar\").note_count.describe())\n",
        "\n",
        "train_data[train_data.note_count<=50].groupby(\"control_cat\").note_count.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zah6FYvHEG5U"
      },
      "source": [
        "train_data.groupby(\"prompt_finalchar\")[['is_orig', 'is_ask', 'is_reply', 'is_reblog']].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8kKfp1pET21"
      },
      "source": [
        "# drop some malformed stuff -- 2/8/21\n",
        "\n",
        "filt_reblog_dash_miscategorized_as_orig = (train_data.prompt_finalchar==\"翰\") & (train_data.is_orig==False)\n",
        "train_data = train_data[~filt_reblog_dash_miscategorized_as_orig]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IXXTuLJBLG3"
      },
      "source": [
        "train_data[\"log_id\"] = train_data[\"id\"].apply(np.log)\n",
        "train_data[\"log_id_qcut\"] = pd.qcut(train_data.log_id, 10, labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiW5_L1eBUax"
      },
      "source": [
        "temporally_ordered_train_data = train_data.sort_values(\"timestamp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgBZsv83BVQP"
      },
      "source": [
        "temporally_ordered_train_data_orig = temporally_ordered_train_data[temporally_ordered_train_data.prompt_finalchar == \"翰\"]# .reset_index()\n",
        "temporally_ordered_train_data_resp_a = temporally_ordered_train_data[temporally_ordered_train_data.prompt_finalchar == \"域a\"]# .reset_index()\n",
        "temporally_ordered_train_data_resp_r = temporally_ordered_train_data[temporally_ordered_train_data.prompt_finalchar == \"域r\"]# .reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO5zrun2BV80"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def non_overlapping_ma(array, width=31):\n",
        "  return pd.Series([np.average(array[ix:ix+width], )\n",
        "   for ix in range(0, len(array), width)\n",
        "   if len(array)-ix > width/2])\n",
        "\n",
        "def do_rolling_quantiles(temporally_ordered_train_data__, \n",
        "                         window_width=140, \n",
        "                         skip_n_most_recent=40,\n",
        "                         allow_partial_windows=False,\n",
        "                         window_frac_left=0.8,\n",
        "                         unit=\"days\",\n",
        "                         ):\n",
        "  temporally_ordered_train_data_ = deepcopy(temporally_ordered_train_data__)\n",
        "  if unit == \"days\":\n",
        "    temporally_ordered_train_data_ = temporally_ordered_train_data_.set_index(\"timestamp\")\n",
        "  else:\n",
        "    temporally_ordered_train_data_ = temporally_ordered_train_data_.reset_index()\n",
        "\n",
        "  window_halfw = window_width//2\n",
        "  rolling_quantiles = {}\n",
        "  rolling_advantages = {}\n",
        "  rolling_medians = {}\n",
        "  rolling_counts = {}\n",
        "\n",
        "  if window_frac_left is not None:\n",
        "    window_shift_left = -1*int(window_frac_left*window_width)\n",
        "    window_shift_right = window_width + window_shift_left\n",
        "  else:\n",
        "    window_shift_left = -window_halfw\n",
        "    window_shift_right = window_halfw\n",
        "\n",
        "  if unit == \"days\":\n",
        "    day_dt = pd.Timedelta(days=1)\n",
        "    window_shift_left = window_shift_left * day_dt\n",
        "    window_shift_right = window_shift_right * day_dt\n",
        "    last_ix_allowed = temporally_ordered_train_data_.index.max() - (skip_n_most_recent*day_dt)\n",
        "  else:\n",
        "    last_ix_allowed = len(temporally_ordered_train_data_) - skip_n_most_recent\n",
        "\n",
        "  if allow_partial_windows:\n",
        "    if unit == \"days\":\n",
        "      ixs = temporally_ordered_train_data_[:last_ix_allowed].index\n",
        "    else:\n",
        "      ixs = temporally_ordered_train_data_.index[:last_ix_allowed]\n",
        "  else:\n",
        "    if unit == \"days\":\n",
        "      _first = temporally_ordered_train_data_.index.min()\n",
        "      ixs = temporally_ordered_train_data_.loc[(_first-window_shift_left):(last_ix_allowed-window_shift_right)].index\n",
        "    else:\n",
        "      ixs = temporally_ordered_train_data_.index[(-window_shift_left):(last_ix_allowed-window_shift_right)]\n",
        "\n",
        "  if unit == \"days\":\n",
        "    _first = temporally_ordered_train_data_.index.min()\n",
        "    _last = temporally_ordered_train_data_.index.max()\n",
        "  else:\n",
        "    _first = 0\n",
        "    _last = len(temporally_ordered_train_data_)-1\n",
        "  print(f\"using ({ixs.min()} to {ixs.max()}) of ({_first} to {_last})\")\n",
        "\n",
        "  for ix in ixs:\n",
        "    point_first_try = temporally_ordered_train_data_.loc[ix, :]\n",
        "    if len(point_first_try.shape)>1:\n",
        "      points = [point_first_try.iloc[i, :] for i in range(len(point_first_try))]\n",
        "    else:\n",
        "      points = [point_first_try]\n",
        "    window = temporally_ordered_train_data_.loc[ix+window_shift_left:ix+window_shift_right, 'note_count']\n",
        "    for point in points:\n",
        "      try:\n",
        "        rolling_quantiles[point[\"id\"]] = (point[\"note_count\"]>=window).mean()\n",
        "      except Exception as e:\n",
        "        display(ix)\n",
        "        display(point_first_try)\n",
        "        display(point)\n",
        "        display(window)\n",
        "        raise e\n",
        "      rolling_advantages[point[\"id\"]] = (point[\"note_count\"]-window).mean()\n",
        "      rolling_medians[point[\"id\"]] = np.median(window)\n",
        "      rolling_counts[point[\"id\"]] = len(window)\n",
        "\n",
        "  rolling_quantiles = pd.Series(rolling_quantiles)\n",
        "  rolling_advantages = pd.Series(rolling_advantages)\n",
        "  rolling_medians = pd.Series(rolling_medians)\n",
        "  rolling_counts = pd.Series(rolling_counts)\n",
        "  return rolling_quantiles, rolling_advantages, rolling_medians, rolling_counts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb0jKJyiBWsk"
      },
      "source": [
        "allow_partial_windows = False\n",
        "window_frac_left = 0.9 # None\n",
        "\n",
        "# ID BASED\n",
        "\n",
        "# window_width_resp_a = 100\n",
        "# skip_n_most_recent_resp_a = 0 # 20\n",
        "\n",
        "# DAY BASED\n",
        "\n",
        "window_width_resp_a = 14\n",
        "skip_n_most_recent_resp_a = 1/12 # 20\n",
        "\n",
        "rolling_quantiles_resp_a, rolling_advantages_resp_a, rolling_medians_resp_a, rolling_counts_resp_a = do_rolling_quantiles(\n",
        "    temporally_ordered_train_data_resp_a,\n",
        "    window_width=window_width_resp_a, \n",
        "    skip_n_most_recent=skip_n_most_recent_resp_a,\n",
        "    allow_partial_windows=allow_partial_windows,\n",
        "    window_frac_left=window_frac_left,\n",
        "    )\n",
        "\n",
        "train_data_resp_a_ = temporally_ordered_train_data_resp_a.set_index(\"id\").loc[rolling_quantiles_resp_a.index]\n",
        "train_data_resp_a_[\"rolling_quantile\"] = rolling_quantiles_resp_a\n",
        "train_data_resp_a_[\"rolling_advantage\"] = rolling_advantages_resp_a\n",
        "train_data_resp_a_[\"rolling_count\"] = rolling_counts_resp_a\n",
        "\n",
        "# non_overlapping_ma(rolling_quantiles_resp_a, width=len(rolling_quantiles_resp_a)//10+1).plot(lw=1, ls='--', marker='.', markersize=5, figsize=(10, 6));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlQtXZ_TBXqS"
      },
      "source": [
        "allow_partial_windows = False\n",
        "window_frac_left = 0.9 # None\n",
        "\n",
        "# ID BASED\n",
        "\n",
        "# window_width_resp_r = 100\n",
        "# skip_n_most_recent_resp_r = 0 # 10\n",
        "\n",
        "# DAY BASED\n",
        "\n",
        "window_width_resp_r = 14\n",
        "skip_n_most_recent_resp_r = 1/12 # 20\n",
        "\n",
        "rolling_quantiles_resp_r, rolling_advantages_resp_r, rolling_medians_resp_r, rolling_counts_resp_r = do_rolling_quantiles(\n",
        "    temporally_ordered_train_data_resp_r,\n",
        "    window_width=window_width_resp_r, \n",
        "    skip_n_most_recent=skip_n_most_recent_resp_r,\n",
        "    allow_partial_windows=allow_partial_windows,\n",
        "    window_frac_left=window_frac_left,\n",
        "    )\n",
        "\n",
        "train_data_resp_r_ = temporally_ordered_train_data_resp_r.set_index(\"id\").loc[rolling_quantiles_resp_r.index]\n",
        "train_data_resp_r_[\"rolling_quantile\"] = rolling_quantiles_resp_r\n",
        "train_data_resp_r_[\"rolling_advantage\"] = rolling_advantages_resp_r\n",
        "train_data_resp_r_[\"rolling_count\"] = rolling_counts_resp_r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Eidg7MBYsv"
      },
      "source": [
        "allow_partial_windows = False\n",
        "window_frac_left = 0.9 # None\n",
        "\n",
        "# ID BASED \n",
        "\n",
        "# window_width_orig = 100\n",
        "# skip_n_most_recent_orig = 0 # 10\n",
        "\n",
        "# DAY BASED\n",
        "\n",
        "window_width_orig = 42 # 14\n",
        "skip_n_most_recent_orig = 1/12 # 10\n",
        "\n",
        "rolling_quantiles_orig, rolling_advantages_orig, rolling_medians_orig, rolling_counts_orig = do_rolling_quantiles(\n",
        "    temporally_ordered_train_data_orig,\n",
        "    window_width=window_width_orig, \n",
        "    skip_n_most_recent=skip_n_most_recent_orig,\n",
        "    allow_partial_windows=allow_partial_windows,\n",
        "    window_frac_left=window_frac_left,\n",
        "    )\n",
        "\n",
        "train_data_orig_ = temporally_ordered_train_data_orig.set_index(\"id\").loc[rolling_quantiles_orig.index]\n",
        "train_data_orig_[\"rolling_quantile\"] = rolling_quantiles_orig\n",
        "train_data_orig_[\"rolling_advantage\"] = rolling_advantages_orig\n",
        "train_data_orig_[\"rolling_count\"] = rolling_counts_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSnPyQwRBZfY"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# notes_key = \"note_count\" \n",
        "notes_key = \"rolling_quantile\"\n",
        "train_data_modified_list = []\n",
        "\n",
        "drop_midrange = True\n",
        "smaller_midrange_dropped = False\n",
        "\n",
        "for name, train_data_base in [(\"resp_a\", train_data_resp_a_),\n",
        "                              (\"resp_r\", train_data_resp_r_),\n",
        "                              (\"orig\", train_data_orig_)]:\n",
        "  train_data_ = deepcopy(train_data_base)\n",
        "  \n",
        "  if smaller_midrange_dropped:\n",
        "    MIDRANGE_BOTTOM = np.percentile(train_data_[notes_key], 30)\n",
        "    MIDRANGE_TOP = np.percentile(train_data_[notes_key], 70)\n",
        "    print((MIDRANGE_BOTTOM, MIDRANGE_TOP))\n",
        "  else:\n",
        "    MIDRANGE_BOTTOM = np.percentile(train_data_[notes_key], 25)\n",
        "    MIDRANGE_TOP = np.percentile(train_data_[notes_key], 75)\n",
        "    print((MIDRANGE_BOTTOM, MIDRANGE_TOP))\n",
        "\n",
        "  \n",
        "\n",
        "  train_data_[\"target\"] = (train_data_[notes_key] >= MIDRANGE_TOP).astype(int)\n",
        "  train_data_ = train_data_[(train_data_[notes_key] <= MIDRANGE_BOTTOM) | (train_data_[notes_key] >= MIDRANGE_TOP)]\n",
        "  \n",
        "  train_data_modified_list.append(train_data_)\n",
        "\n",
        "train_data_ = pd.concat(train_data_modified_list, ignore_index=True)\n",
        "\n",
        "stratify = train_data_[\"target\"]\n",
        "\n",
        "model_inputs = train_data_[[\"text\", \"target\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYz3d4mDBafa"
      },
      "source": [
        "# vs 8066 | vs 8059\n",
        "display(model_inputs.target.describe())\n",
        "display(train_data_.groupby(\"prompt_finalchar\").target.agg([\"count\", \"mean\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKfNF7pBbxT"
      },
      "source": [
        "if TRAIN_SENTIMENT:\n",
        "  if not os.path.exists(\"sentiment_train_data.jsonl\"):\n",
        "    !gsutil cp gs://{BUCKET_NAME}/sentiment_train_data.jsonl .\n",
        "  sentiment_train_data = pd.read_json(\"sentiment_train_data.jsonl\",\n",
        "                                      orient=\"records\",\n",
        "                                      lines=True)\n",
        "  train_data_ = sentiment_train_data\n",
        "  train_data_[\"prompt\"] = train_data_.text\n",
        "\n",
        "  prompt_finalchar = train_data_.prompt.apply(lambda s: s[-1])\n",
        "  train_data_[\"prompt_finalchar\"] = prompt_finalchar\n",
        "\n",
        "  train_data_[\"logit_diff\"] = train_data_.pos_logit - train_data_.neg_logit\n",
        "  train_data_[\"target\"] = (train_data_.logit_diff > 0).astype(int)\n",
        "  \n",
        "  train_data_[\"target_p75_generated\"] = None\n",
        "  filter_ = train_data_.p75_generated_logit_diff.notnull()\n",
        "  train_data_.loc[filter_, \"target_p75_generated\"] = (train_data_[filter_].p75_generated_logit_diff > 0).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfHNlIx-Bg6r"
      },
      "source": [
        "if TRAIN_SENTIMENT:\n",
        "  import re\n",
        "  \n",
        "  try:\n",
        "    from transformers.tokenization_roberta import RobertaTokenizer\n",
        "  except ModuleNotFoundError:\n",
        "    !pip install transformers==2.11.0\n",
        "    from transformers.tokenization_roberta import RobertaTokenizer\n",
        "\n",
        "  rtok = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "  def _sc_prep(text):\n",
        "    sanitized_text = re.sub(r\"\\<.*?\\>\", \"\", text)\n",
        "    sanitized_text = rtok.convert_tokens_to_string(rtok.tokenize(sanitized_text)[:200])\n",
        "    return sanitized_text\n",
        "\n",
        "  train_data_[\"text_raw\"] = train_data_.text\n",
        "  train_data_[\"text\"] = \" \" + train_data_.text_raw.apply(_sc_prep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j84Z-UpBhw9"
      },
      "source": [
        "TEST_SIZE = 1/10\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  TEST_SIZE = 0.125"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZCDs0b7tEH4"
      },
      "source": [
        "Munging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eg_pf2ZB7nR"
      },
      "source": [
        "from autoresponder_static_v8 import *\n",
        "GLOBAL_DEBUG = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z5sNaUrB7zJ"
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "def _addmonth(ts):\n",
        "  _month = ts.strftime(\"%B\")\n",
        "  _newmonth = ts.strftime(\"%B\")\n",
        "  while _newmonth == _month:\n",
        "    ts = ts + timedelta(days=14)\n",
        "    _newmonth = ts.strftime(\"%B\")\n",
        "  return ts\n",
        "\n",
        "def fake_v10_timestamps(ts_real, start=None, n=3):\n",
        "  if start is None:\n",
        "    start = datetime.now()\n",
        "  tss = [start]\n",
        "  for i in range(n):\n",
        "    tss.append(_addmonth(tss[-1]))\n",
        "\n",
        "  return [\" \".join(ts_real.split(\" \")[:2]) + ts.strftime(\" %B %Y\") for ts in tss]\n",
        "\n",
        "def one_fake_v10_timestamp(ts_real, n=3):\n",
        "  return np.random.choice(fake_v10_timestamps(ts_real, n=n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPueuPhB8ny"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold, train_test_split\n",
        "\n",
        "V10 = True  #  EXPERIMENTAL\n",
        "TIMESTAMP_FAKING = True  #  EXPERIMENTAL\n",
        "V8 = True\n",
        "\n",
        "TRUNC_DEFER_TO_EST = False\n",
        "\n",
        "IGNORE_REBLOGS = True # was once EXPERIMENTAL\n",
        "\n",
        "CONTINUATION_ONLY = False\n",
        "LAST_USER_INPUT_ONLY = False\n",
        "\n",
        "NO_TAGS = False\n",
        "FORCE_T_CHAR = True\n",
        "INCLUDE_PROMPT_FINALCHAR = True\n",
        "NORMALIZE = True\n",
        "TRUNCATE_AT_RIGHT = False  # EXPERIMENTAL\n",
        "FORUMLIKE = True\n",
        "FORUMLIKE_MODE = \"train\" # was once EXPERIMENTAL\n",
        "EOT_PREPEND = True # was once EXPERIMENTAL\n",
        "\n",
        "if V8:\n",
        "  FORCE_T_CHAR = False\n",
        "  EOT_PREPEND = True\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  CONTINUATION_ONLY = True\n",
        "  NO_TAGS = True\n",
        "  FORCE_T_CHAR = False\n",
        "  INCLUDE_PROMPT_FINALCHAR = False\n",
        "  TRUNCATE_AT_RIGHT = True\n",
        "  TRUNC_DEFER_TO_EST = False\n",
        "\n",
        "def strip_uname_tags(s, prompt, verbose=False):\n",
        "  post, optional_tchar, tagbody = s.partition(T_CHAR)\n",
        "  tags = [t for t in tagbody.split(\"#\") if len(t)>0]\n",
        "\n",
        "  stripped_tags = []\n",
        "  for t in tags:\n",
        "    uname_substr = UNAME_CHAR + t.rstrip(\" \")\n",
        "    uname_substr2 = UNAME_CHAR + \" \" + t.rstrip(\" \")\n",
        "    if uname_substr not in prompt and uname_substr2 not in prompt:\n",
        "        stripped_tags.append(\"#\" + t)\n",
        "    else:\n",
        "      if verbose:\n",
        "        print(f\"found {t} as {uname_substr}\")\n",
        "  \n",
        "  return post + optional_tchar + \"\".join(stripped_tags)\n",
        "\n",
        "if V10:\n",
        "  control_seg_config = CONTROL_SEG_CONFIGS[\"V10\"]\n",
        "  chinese_to_neural_munger = final_munge_before_neural_v10\n",
        "  chinese_to_neural_munger_prompt = chinese_to_neural_munger\n",
        "elif V8:\n",
        "  control_seg_config = CONTROL_SEG_CONFIGS[\"V9\"]\n",
        "  chinese_to_neural_munger = final_munge_before_neural_v8\n",
        "  chinese_to_neural_munger_prompt = chinese_to_neural_munger\n",
        "else:\n",
        "  chinese_to_neural_munger = lambda s: substitute_forumlike(normalize_for_generator(s), shuffle=False, infer_first=False, mode=FORUMLIKE_MODE)\n",
        "  chinese_to_neural_munger_prompt = lambda s: substitute_forumlike(normalize_for_generator(s), shuffle=False, infer_first=False, mode=\"predict\")\n",
        "\n",
        "train_data_for_selection = train_data_.copy()\n",
        "selector_input_continuation = train_data_for_selection.text.apply(lambda s: (s[:-2] if s.endswith(\"<|\") else s))\n",
        "selector_input_prompt = train_data_for_selection.prompt\n",
        "\n",
        "if TIMESTAMP_FAKING and not TRAIN_SENTIMENT:\n",
        "  train_data_for_selection['fake_v10_timestamp'] = train_data_for_selection.v10_timestamp.apply(one_fake_v10_timestamp)\n",
        "\n",
        "if V10 and not TRAIN_SENTIMENT:\n",
        "  timestamps_to_add = train_data_for_selection.fake_v10_timestamp if TIMESTAMP_FAKING else train_data_for_selection.v10_timestamp\n",
        "    \n",
        "  selector_input_continuation = selector_input_continuation + TIME_SIDECHANNEL_CHAR + timestamps_to_add\n",
        "elif V8 and not TRAIN_SENTIMENT:\n",
        "  selector_input_continuation = selector_input_continuation + TIME_SIDECHANNEL_CHAR + train_data_for_selection.v8_timestamp\n",
        "\n",
        "if LAST_USER_INPUT_ONLY:\n",
        "  full = train_data_for_selection.prompt + selector_input_continuation\n",
        "  full = full.apply(chinese_to_neural_munger)\n",
        "\n",
        "  selector_input = full.apply(\n",
        "    lambda s: s[find_control_chars_forumlike(s)[-2:][0][1]:]\n",
        "  )\n",
        "elif CONTINUATION_ONLY:\n",
        "  selector_input = selector_input_continuation.copy()\n",
        "else:\n",
        "  selector_input = selector_input_prompt + selector_input_continuation\n",
        "\n",
        "if CONTINUATION_ONLY and INCLUDE_PROMPT_FINALCHAR:\n",
        "  if FORUMLIKE:\n",
        "    full = train_data_for_selection.prompt + selector_input_continuation\n",
        "    full = full.apply(chinese_to_neural_munger)\n",
        "\n",
        "    selector_input = full.apply(\n",
        "      lambda s: s[last_control_char_forumlike(s, incl_number=False,)[1]:]\n",
        "    )\n",
        "  else:\n",
        "    selector_input = train_data_for_selection.prompt_finalchar + selector_input\n",
        "elif FORUMLIKE and not TRAIN_SENTIMENT and not LAST_USER_INPUT_ONLY:\n",
        "  selector_input_debug = selector_input\n",
        "  selector_input = selector_input.apply(\n",
        "    chinese_to_neural_munger\n",
        "  )\n",
        "if FORUMLIKE and not TRAIN_SENTIMENT:\n",
        "  if V10:\n",
        "    timestamps_to_add = train_data_for_selection.fake_v10_timestamp if TIMESTAMP_FAKING else train_data_for_selection.v10_timestamp\n",
        "    prompt_forumlike_in = selector_input_prompt + TIME_SIDECHANNEL_CHAR + timestamps_to_add\n",
        "  elif V8:\n",
        "    prompt_forumlike_in = selector_input_prompt + TIME_SIDECHANNEL_CHAR + train_data_for_selection.v8_timestamp\n",
        "  else:\n",
        "    prompt_forumlike_in = selector_input_prompt\n",
        "  prompt_forumlike = prompt_forumlike_in.apply(\n",
        "    chinese_to_neural_munger_prompt\n",
        "  )\n",
        "  if LAST_USER_INPUT_ONLY:\n",
        "    prompt_forumlike = prompt_forumlike.apply(\n",
        "      lambda s: s[find_control_chars_forumlike(s)[-2:][0][1]:]\n",
        "    )\n",
        "  train_data_for_selection[\"prompt_forumlike\"] = prompt_forumlike  \n",
        "\n",
        "\n",
        "if FORCE_T_CHAR:\n",
        "  selector_input = selector_input.apply(lambda s: s if T_CHAR in s else s+T_CHAR)\n",
        "\n",
        "if NO_TAGS:\n",
        "  selector_input = selector_input.apply(lambda s: s.partition(T_CHAR)[0].rstrip(\"\\n\\ufffa\\ufffb \") + s.partition(T_CHAR)[1])\n",
        "else:\n",
        "  selector_input.iloc[:] = [strip_uname_tags(s, prompt) for s, prompt in zip(selector_input, train_data_for_selection.prompt)]\n",
        "  selector_input = selector_input.apply(lambda s: s.rstrip(\"\\n\\ufffa\\ufffb \"))\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  selector_input = selector_input.apply(lambda s: re.sub(r\"\\<.*?\\>\", \"\", s))  # same norm for allen sentiment\n",
        "\n",
        "if NORMALIZE and ((not FORUMLIKE) or TRAIN_SENTIMENT):\n",
        "  selector_input = selector_input.apply(normalize_for_generator)\n",
        "  \n",
        "if EOT_PREPEND:\n",
        "  selector_input = EOT_FULL + selector_input\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  selector_input = selector_input.apply(lambda s: enc.decode(enc.encode(s)[:256-1]))\n",
        "\n",
        "train_data_for_selection[\"selector_input\"] = selector_input\n",
        "\n",
        "train_data_for_selection[\"prefix\"] = train_data_for_selection.prompt.apply(\n",
        "    lambda s: [item for item in s.split(UNAME_CHAR) if len(item)>0][0]\n",
        "    )\n",
        "\n",
        "# train/test with groups\n",
        "train_data_for_selection.loc[train_data_for_selection.prompt_finalchar == ORIG_POST_CHAR_CHINESE, \"prefix\"] = \\\n",
        "\"域\\n\\n\" + train_data_for_selection.loc[train_data_for_selection.prompt_finalchar == ORIG_POST_CHAR_CHINESE, \"text\"]\n",
        "\n",
        "full_train_data_for_selection = deepcopy(train_data_for_selection)\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  train_data_for_selection, test_data_for_selection = train_test_split(\n",
        "      train_data_for_selection, test_size=TEST_SIZE,\n",
        "      stratify=train_data_for_selection.target)  # stratifies on sign of sentiment -- seems OK\n",
        "else:\n",
        "  train_data_for_selection = train_data_for_selection[\n",
        "    train_data_for_selection.prompt_finalchar!=\"域r\"]\n",
        "\n",
        "  stratifier = train_data_for_selection.prompt_finalchar + train_data_for_selection.target.apply(str)\n",
        "  train_data_for_selection, test_data_for_selection = train_test_split(train_data_for_selection, stratify=stratifier, test_size=TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRlCOutDtHy"
      },
      "source": [
        "if not TRAIN_SENTIMENT:\n",
        "  display(train_data_for_selection.groupby([\"prompt_finalchar\", \"target\"]).selector_input.count())\n",
        "  display(test_data_for_selection.groupby([\"prompt_finalchar\", \"target\"]).selector_input.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfv1sgzHF44D"
      },
      "source": [
        "import selector_nn\n",
        "import selector_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ew8zXqCGSAU"
      },
      "source": [
        "train_data_for_selection[\"n_tokens\"] = train_data_for_selection[\"selector_input\"].apply(lambda s: len(enc.encode(s)))\n",
        "if not TRAIN_SENTIMENT:\n",
        "  train_data_for_selection[\"prompt_end_ntoks\"] = train_data_for_selection[\"prompt_forumlike\"].apply(lambda s: len(enc.encode(s)))\n",
        "\n",
        "train_data_for_selection_final = selector_estimator.reshuffle_batches(train_data_for_selection, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFFaBa-yGS8A"
      },
      "source": [
        "test_data_for_selection[\"n_tokens\"] = test_data_for_selection[\"selector_input\"].apply(lambda s: len(enc.encode(s)))\n",
        "if not TRAIN_SENTIMENT:\n",
        "  test_data_for_selection[\"prompt_end_ntoks\"] = test_data_for_selection[\"prompt_forumlike\"].apply(lambda s: len(enc.encode(s)))\n",
        "\n",
        "test_data_for_selection_final = selector_estimator.reshuffle_batches(test_data_for_selection, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA3grPSmHMH8"
      },
      "source": [
        "print(train_data_for_selection_final.shape)\n",
        "print(test_data_for_selection_final.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj9Uo0D4Hhnj"
      },
      "source": [
        "fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFJ3bNKFJ31w"
      },
      "source": [
        "lengths_to_try = [int(train_data_for_selection_final.n_tokens.quantile(q)) for q in [0.95, 0.97, 0.975, 0.98,]] + [825]\n",
        "\n",
        "lengths_to_try"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0uDTHOFHM_N"
      },
      "source": [
        "EXPERIMENTAL = False\n",
        "EXECUTIVE_DECISIONS_ON = True # !\n",
        "\n",
        "CALIB_DEFER_TO_EST = True\n",
        "\n",
        "TRY_SHORT_LENGTH = True\n",
        "TRY_SHORT_LENGTH_IX = -4\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  SENTIMENT_LOGIT_SUPERVISION = True\n",
        "  SENTIMENT_2D_LOGIT_SUPERVISION = False\n",
        "  SENTIMENT_P75 = False\n",
        "\n",
        "  est_config = {\n",
        "          \"layer_nums\": (7, 23),\n",
        "          \"huber\": True,\n",
        "          \"show_batch_stats\": False,\n",
        "          \"orth_init\": True,\n",
        "          \"use_mlp\": True,\n",
        "          \"resid_mlp\": True,\n",
        "          \"mlp_ratio\": 1.,\n",
        "          \"epochs\": 3,\n",
        "          \"acti_dropout\": 0.15,\n",
        "          \"res_dropout\": 0.1,\n",
        "          \"attn_dropout\": 0.1,\n",
        "          \"weight_decay\": 0.01,\n",
        "          \"base_lr\": 5e-5,\n",
        "          \"min_lr_frac\": 0.05,\n",
        "          \"warmup_ratio\": 0.1,\n",
        "          \"warm_resets\": False,\n",
        "          \"grad_clip\": 1000.,\n",
        "          \"length\": 204,\n",
        "          \"evaluate_during_training\": False,\n",
        "          \"supervise_logits\": SENTIMENT_LOGIT_SUPERVISION,\n",
        "          \"supervise_only_logit_diff\": not SENTIMENT_2D_LOGIT_SUPERVISION,\n",
        "          \"use_only_logit_diff\": False,\n",
        "          \"calibrate\": False,\n",
        "          \"calibration_split_type\": \"tts\",\n",
        "          \"calibration_val_size\": 0.01,\n",
        "          \"init_default_gain\": 1,\n",
        "          \"n_head\": 32}\n",
        "\n",
        "else:\n",
        "  est_config = {\"layer_nums\": (7, 23 ),\n",
        "  \"use_mlp\": True,\n",
        "  \"resid_mlp\": True,\n",
        "  \"mlp_ratio\": 2,\n",
        "  \"flooding\": True,\n",
        "  \"flood_level\": 0.0667555,\n",
        "  \"orth_init\": True,\n",
        "  \"stop_early\": False,\n",
        "  \"warmup_ratio\": 0.0318089,\n",
        "  \"epochs\": 4,\n",
        "  \"res_dropout\": 0.0853267,\n",
        "  \"acti_dropout\": 0.279725,\n",
        "  \"attn_dropout\": 0.115453,\n",
        "  \"weight_decay\": 0.05,\n",
        "  \"base_lr\": 2.58815e-5,\n",
        "  \"min_lr_frac\": 0.435668,\n",
        "  \"m_mul\": 0.667,\n",
        "  \"grad_clip\": 1000.,\n",
        "  \"length\": 825,\n",
        "  \"supervise_logits\": False,\n",
        "  \"supervise_only_logit_diff\": False,\n",
        "  \"calibrate\": True,\n",
        "  \"calibration_split_type\": \"ttsp\",\n",
        "  \"calibrate_prefixes_separately\": False,\n",
        "  \"calibration_val_size\": 1/8,\n",
        "  \"use_only_logit_diff\": False,\n",
        "  \"init_default_gain\": 1.,\n",
        "  \"n_head\": 40,\n",
        "  }\n",
        "\n",
        "  if EXECUTIVE_DECISIONS_ON:\n",
        "    est_config['epochs'] = 3 # ! (1/26/21)\n",
        "    est_config['calibration_val_size'] = 0.1 # ! (1/27/21)\n",
        "\n",
        "    est_config['mlp_ratio'] = 3\n",
        "    est_config['acti_dropout'] = 0\n",
        "    est_config['res_dropout'] = 0.1\n",
        "    est_config['attn_dropout'] = 0.1\n",
        "    est_config['weight_decay'] = 0.025\n",
        "\n",
        "    est_config['min_lr_frac'] = 0.25\n",
        "    est_config['warmup_ratio'] = 0.05\n",
        "\n",
        "  if EXPERIMENTAL:\n",
        "    est_config['additional_full_blocks'] = 1\n",
        "    est_config['layer_nums'] = (23,)\n",
        "\n",
        "if TRY_SHORT_LENGTH and not TRAIN_SENTIMENT:\n",
        "  est_config['length'] = lengths_to_try[TRY_SHORT_LENGTH_IX]\n",
        "  print(f\"trying length: {est_config['length']}\\n\")\n",
        "  print(\"cases clipped w/ this length\")\n",
        "  display((train_data_for_selection.n_tokens > est_config['length']).agg(['sum', 'mean']))\n",
        "  print(f\"\\nn_head: {est_config['n_head']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUdOD2wnHiG2"
      },
      "source": [
        "import tflex\n",
        "\n",
        "est=selector_estimator.SelectorEstimatorFromCkpt(\n",
        "    cleanup_on_exception=False,\n",
        "    persist_variables=False,\n",
        "    ckpt=tflex.latest_checkpoint(ckpt_dir),\n",
        "    enc=enc,\n",
        "    selection_tok=enc.encode(\"<|endoftext|>\")[-1],\n",
        "    **est_config\n",
        "    )\n",
        "\n",
        "if TRAIN_SENTIMENT and SENTIMENT_LOGIT_SUPERVISION:\n",
        "  avg_loss_beta = 0.995\n",
        "  if True:#SENTIMENT_LOGIT_SUPERVISION:\n",
        "    final_fit_X = train_data_for_selection_final.drop([\"neg_logit\", \"pos_logit\"], axis=1)\n",
        "    final_fit_target = train_data_for_selection_final[[\"neg_logit\", \"pos_logit\"]]\n",
        "  else:\n",
        "    final_fit_X = train_data_for_selection_final.drop(\"logit_diff\", axis=1)\n",
        "    final_fit_target = train_data_for_selection_final.logit_diff\n",
        "else:\n",
        "  avg_loss_beta = 0.99\n",
        "  final_fit_X = train_data_for_selection_final.drop(\"target\", axis=1)\n",
        "  final_fit_target = train_data_for_selection_final.target\n",
        "\n",
        "try:\n",
        "  est.cleanup()\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD10XCrvHlKA"
      },
      "source": [
        "est.fit(final_fit_X, final_fit_target, avg_loss_beta=avg_loss_beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Goy8cVtH76"
      },
      "source": [
        "Evaluate\n",
        "\n",
        "(This is very old code and could be cleaned up)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJqGWhiiHmGo"
      },
      "source": [
        "sess = est.session_\n",
        "selection_step_train = est.selection_step_train_\n",
        "selection_step_eval = est.selection_step_eval_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfmstU0WLKNW"
      },
      "source": [
        "lr = est.lr_\n",
        "opt = est.opt_\n",
        "\n",
        "opt_apply = est.opt_apply_\n",
        "select_loss = est.select_loss_\n",
        "\n",
        "select_logits_train = est.select_logits_train_\n",
        "select_logits_eval = est.select_logits_eval_\n",
        "\n",
        "if TRAIN_SENTIMENT and SENTIMENT_LOGIT_SUPERVISION and not SENTIMENT_2D_LOGIT_SUPERVISION:\n",
        "  select_target = est.select_target_logit_diff_\n",
        "else:\n",
        "  select_target = est.select_target_\n",
        "\n",
        "context_for_h = est.context_for_h_\n",
        "global_step = est.global_step_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PzSHNATLLvT"
      },
      "source": [
        "import scipy.special\n",
        "\n",
        "def predict_select(data_batch, threshold=0.5, truncate_at_right=TRUNCATE_AT_RIGHT,\n",
        "                   ):\n",
        "  if len(data_batch) != batch_size_for_h:\n",
        "    raise ValueError(\"badlength\")\n",
        "  feed_dict = est._feed_from_batch(data_batch, scope=est.select_scope_eval_)\n",
        "\n",
        "  with sess.as_default():\n",
        "    logits = sess.run(select_logits_eval, feed_dict=feed_dict)\n",
        "\n",
        "  probs = scipy.special.softmax(logits, axis=1)\n",
        "  results = {\"logits\": logits, \"probs\": probs}\n",
        "  results[\"preds\"] = probs[:, 1]>threshold\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_aJDfLULM1U"
      },
      "source": [
        "import time \n",
        "\n",
        "def eval_selection(data, steps=None, start_ix=0, silent=False, truncate_at_right=TRUNCATE_AT_RIGHT):\n",
        "  all_preds = []\n",
        "  all_probs = []\n",
        "  all_target_logits = []\n",
        "  all_targets = []\n",
        "  all_logits = []\n",
        "  all_row_ix = []\n",
        "\n",
        "  if steps is None:\n",
        "    steps = len(data)//est.batch_size\n",
        "\n",
        "  row_ix = start_ix*est.batch_size\n",
        "  \n",
        "  step_iter = tqdm(list(range(start_ix, steps))) if silent else range(start_ix, steps)\n",
        "  for step_ix in step_iter:\n",
        "    data_batch = data.iloc[row_ix:row_ix + est.batch_size, :]\n",
        "    \n",
        "    t1 = time.time()\n",
        "    try:\n",
        "      results_batch = est._predict_select(data_batch, disable_calibration=False)\n",
        "    except Exception as e:\n",
        "      if not silent:\n",
        "        print(f\"skipping batch ({e})\")\n",
        "      continue\n",
        "    t2 = time.time()\n",
        "    tdiff = t2 - t1\n",
        "\n",
        "    if TRAIN_SENTIMENT:\n",
        "      all_target_logits.extend(data_batch[[\"neg_logit\", \"pos_logit\"]].values)\n",
        "      all_targets.extend((data_batch.pos_logit > data_batch.neg_logit).astype(int))\n",
        "    \n",
        "    else:\n",
        "      all_targets.extend(data_batch.target.values)\n",
        "    all_preds.extend(results_batch[\"preds\"])\n",
        "    all_probs.extend(results_batch[\"probs\"])\n",
        "    all_logits.extend(results_batch[\"logits\"])\n",
        "    all_row_ix.extend(list(range(row_ix, row_ix + est.batch_size)))\n",
        "\n",
        "    accs = np.array(all_preds) == np.array(all_targets)\n",
        "    avg_acc = accs.mean()\n",
        "\n",
        "    tp = (np.array(all_targets)>0).sum()\n",
        "    pp = (np.array(all_preds)>0).sum()\n",
        "\n",
        "    assert len(all_targets) == len(all_preds)\n",
        "\n",
        "    if not silent:\n",
        "      print(f\"{step_ix}/{steps} | {tdiff:.2f}s | acc={avg_acc:.4f} | {tp}/{len(all_targets)} true pos | {pp}/{len(all_targets)} pred pos\")\n",
        "      print(\"\\n--------------\\n\")\n",
        "\n",
        "    row_ix += est.batch_size\n",
        "\n",
        "  all_probs = np.stack(all_probs)[:, 1]\n",
        "  return all_preds, all_probs, all_targets, all_logits, all_row_ix, all_target_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiH8dTwJLN6q"
      },
      "source": [
        "all_preds, all_probs, all_targets, all_logits, all_row_ix, all_target_logits = eval_selection(\n",
        "    test_data_for_selection_final, start_ix=0\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7DXp5pn2Pz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "all_logits, all_target_logits = np.array(all_logits), np.array(all_target_logits)\n",
        "all_targets = np.asarray(all_targets)\n",
        "all_preds = np.asarray(all_preds)\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  all_logit_sums = (all_logits[:, 1] + all_logits[:, 0])\n",
        "  all_target_logit_sums = (all_target_logits[:, 1] + all_target_logits[:, 0])\n",
        "\n",
        "  all_logit_diffs = (all_logits[:, 1] - all_logits[:, 0])\n",
        "  all_target_logit_diffs = (all_target_logits[:, 1] - all_target_logits[:, 0])\n",
        "\n",
        "  print(((all_logits[:, 1] > all_logits[:, 0]) == (all_target_logits[:, 1] > all_target_logits[:, 0])).mean())\n",
        "\n",
        "  print(((all_target_logits - all_logits)**2).mean())\n",
        "\n",
        "  display(pd.Series((all_target_logit_diffs - all_logit_diffs)**2).describe())\n",
        "\n",
        "  display(pd.Series(all_target_logit_diffs - all_logit_diffs).describe())\n",
        "\n",
        "  FIGSIZE = (16, 6)\n",
        "  MS = 3\n",
        "  ALPHA = 0.5\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=FIGSIZE)\n",
        "  axes[0].scatter(all_target_logit_diffs, all_logit_diffs, s=MS, alpha=ALPHA)\n",
        "\n",
        "  axes[0].plot(\n",
        "      [all_target_logit_diffs.min(), all_target_logit_diffs.max()],\n",
        "      [all_target_logit_diffs.min(), all_target_logit_diffs.max()],\n",
        "      c='r', ls='--'\n",
        "  )\n",
        "\n",
        "  axes[0].axhline(0, c='g', ls='--')\n",
        "  axes[0].axvline(0, c='g', ls='--')\n",
        "  axes[0].set_title(\"logit diff\")\n",
        "\n",
        "  axes[1].scatter(all_target_logit_sums, all_logit_sums, s=MS, alpha=ALPHA)\n",
        "  axes[1].plot(\n",
        "      [all_target_logit_sums.min(), all_target_logit_sums.max()],\n",
        "      [all_target_logit_sums.min(), all_target_logit_sums.max()],\n",
        "      c='r', ls='--'\n",
        "  )\n",
        "  axes[1].set_title(\"logit sum\")\n",
        "\n",
        "  plt.tight_layout();\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=FIGSIZE)\n",
        "\n",
        "  axes[0].scatter(all_target_logits[:, 0], all_logits[:, 0], s=MS, alpha=ALPHA)\n",
        "\n",
        "  axes[0].plot(\n",
        "      [all_target_logits[:, 0].min(), all_target_logits[:, 0].max()],\n",
        "      [all_target_logits[:, 0].min(), all_target_logits[:, 0].max()],\n",
        "      c='r', ls='--'\n",
        "  )\n",
        "  axes[0].set_title(\"neg logit\")\n",
        "\n",
        "  axes[1].scatter(all_target_logits[:, 1], all_logits[:, 1], s=MS, alpha=ALPHA)\n",
        "\n",
        "  axes[1].plot(\n",
        "      [all_target_logits[:, 1].min(), all_target_logits[:, 1].max()],\n",
        "      [all_target_logits[:, 1].min(), all_target_logits[:, 1].max()],\n",
        "      c='r', ls='--'\n",
        "  )\n",
        "  axes[1].set_title(\"pos logit\")\n",
        "\n",
        "  plt.tight_layout();\n",
        "\n",
        "  review_data = test_data_for_selection_final.iloc[:len(all_probs)].copy()\n",
        "  review_data[\"logit_diff\"] = all_target_logit_diffs\n",
        "  review_data[\"logit_diff_pred\"] = all_logit_diffs\n",
        "\n",
        "  review_data[\"err\"] = review_data.logit_diff - review_data.logit_diff_pred\n",
        "  review_data[\"sq_err\"] = review_data.err.values**2\n",
        "\n",
        "  with pd.option_context(\"display.max_rows\", 999, \"display.max_colwidth\", 200):\n",
        "    display(review_data.sort_values(\"sq_err\", ascending=False)[[\"selector_input\", \"logit_diff\", \"logit_diff_pred\", \"err\", \"sq_err\"]].iloc[:20, :])\n",
        "\n",
        "  with pd.option_context(\"display.max_rows\", 999, \"display.max_colwidth\", 200):\n",
        "    display(review_data.sort_values(\"sq_err\", ascending=True)[[\"selector_input\", \"logit_diff\", \"logit_diff_pred\", \"err\", \"sq_err\"]].iloc[:20, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOJrGs3hLPA0"
      },
      "source": [
        "# new\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n",
        "\n",
        "bins=np.linspace(0., 1., 7)\n",
        "\n",
        "ax[0, 0].hist(all_probs[test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]!=ORIG_POST_CHAR_CHINESE], bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
        "ax[0, 0].set_xticks(bins)\n",
        "ax[0, 0].set_title(\"resp, raw\")\n",
        "\n",
        "_, bins, _ = ax[0, 1].hist(all_probs[(test_data_for_selection_final.target.iloc[:len(all_probs)] < 0.5) & \n",
        "                                     (test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]!=ORIG_POST_CHAR_CHINESE)], bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
        "ax[0, 1].hist(all_probs[(test_data_for_selection_final.target.iloc[:len(all_probs)] > 0.5) &\n",
        "                       (test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]!=ORIG_POST_CHAR_CHINESE)], bins=bins, alpha=0.5, density=False, edgecolor='k');\n",
        "ax[0, 1].set_xticks(bins);\n",
        "ax[0, 1].set_title(\"resp, raw\")\n",
        "\n",
        "# orig\n",
        "ax[1, 0].hist(all_probs[test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]==ORIG_POST_CHAR_CHINESE], bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
        "ax[1, 0].set_xticks(bins)\n",
        "ax[1, 0].set_title(\"orig, raw\")\n",
        "\n",
        "_, bins, _ = ax[1, 1].hist(all_probs[(test_data_for_selection_final.target.iloc[:len(all_probs)] < 0.5) & \n",
        "                                     (test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]==ORIG_POST_CHAR_CHINESE)], bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
        "ax[1, 1].hist(all_probs[(test_data_for_selection_final.target.iloc[:len(all_probs)] > 0.5) &\n",
        "                       (test_data_for_selection_final.prompt_finalchar.iloc[:len(all_probs)]==ORIG_POST_CHAR_CHINESE)], bins=bins, alpha=0.5, density=False, edgecolor='k');\n",
        "ax[1, 1].set_xticks(bins);\n",
        "ax[1, 1].set_title(\"orig, raw\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uETGtlo6LSQL"
      },
      "source": [
        "# new\n",
        "from sklearn.metrics import brier_score_loss, average_precision_score, accuracy_score, log_loss, confusion_matrix\n",
        "\n",
        "review_data = test_data_for_selection_final.iloc[:len(all_probs)].copy()\n",
        "review_data[\"prob\"] = all_probs\n",
        "review_data[\"neg_logits\"] = [l[0] for l in all_logits]\n",
        "review_data[\"pos_logits\"] = [l[1] for l in all_logits]\n",
        "review_data[\"logit_diffs\"] = review_data.pos_logits - review_data.neg_logits\n",
        "\n",
        "err_size = (review_data.prob - review_data.target).apply(np.abs)\n",
        "review_data_ = deepcopy(review_data)\n",
        "review_data_[\"err_size\"] = err_size\n",
        "\n",
        "review_data_[\"is_orig\"] = review_data_.prompt_finalchar==ORIG_POST_CHAR_CHINESE\n",
        "\n",
        "for pfc, g in review_data_.groupby(\"prompt_finalchar\"):\n",
        "  print(f\"pfc={pfc} ({len(g)}/{len(review_data_)})\")\n",
        "  baserate_acc = max(g.target.mean(), 1.-g.target.mean())\n",
        "  baserate_brier = brier_score_loss(g.target, [g.target.mean() for _ in range(len(g.target))])\n",
        "  baserate_AP = average_precision_score(g.target, [g.target.mean() for _ in range(len(g.target))])\n",
        "\n",
        "  print(f\"acc:   {accuracy_score(g.target, g.prob>0.5):.3f} (vs {baserate_acc:.3f})\")\n",
        "  print(f\"brier: {brier_score_loss(g.target, g.prob):.3f} (vs {baserate_brier:.3f})\")\n",
        "  print(f\"AP:    {average_precision_score(g.target, g.prob):.3f} (vs {baserate_AP:.3f})\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ8xMQBALTba"
      },
      "source": [
        "# new \n",
        "from sklearn.metrics import brier_score_loss, average_precision_score, accuracy_score, log_loss, confusion_matrix\n",
        "\n",
        "baserate_acc = max(test_data_for_selection_final.target.mean(), 1.-test_data_for_selection_final.target.mean())\n",
        "baserate_brier = brier_score_loss(test_data_for_selection_final.target, [test_data_for_selection_final.target.mean() for _ in range(len(test_data_for_selection_final.target))])\n",
        "baserate_AP = average_precision_score(test_data_for_selection_final.target, [test_data_for_selection_final.target.mean() for _ in range(len(test_data_for_selection_final.target))])\n",
        "\n",
        "print(f\"acc:   {accuracy_score(all_targets, all_preds):.3f} (vs {baserate_acc:.3f})\")\n",
        "print(f\"brier: {brier_score_loss(all_targets, all_probs):.3f} (vs {baserate_brier:.3f})\")\n",
        "print(f\"AP:    {average_precision_score(all_targets, all_probs):.3f} (vs {baserate_AP:.3f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QGrDYYYLc_i"
      },
      "source": [
        "save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19zcLX66LUhj"
      },
      "source": [
        "majorverson = \"v10\"\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  minorversion=\"v2\"  # manually updated each re-train\n",
        "else:\n",
        "  minorversion=\"v8\"  # manually updated each re-train\n",
        "\n",
        "if TRAIN_SENTIMENT:\n",
        "  save_path = f\"sentiment/{majorverson}/{minorversion}/\"\n",
        "  gs_path = f\"gs://{BUCKET_NAME}/ar_model_v10/v10_sentiment/\"\n",
        "else:\n",
        "   save_path = f\"selector/{majorverson}/{minorversion}/\"\n",
        "   gs_path = f\"gs://{BUCKET_NAME}/ar_model_v10/v10_selector/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0f7Q80uLeEv"
      },
      "source": [
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "else:\n",
        "  print(f\"warning: save_path already exists\")\n",
        "\n",
        "print(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWE9KeYBLhx3"
      },
      "source": [
        "est.save(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsTcKEyQivvI"
      },
      "source": [
        "%ls -lha {save_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15g9f_bunFj"
      },
      "source": [
        "!gsutil -m cp -R {save_path} {gs_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpgH4y-BkkIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMNDQwGdkln_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}